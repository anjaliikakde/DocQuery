# RAG: Multi-Document Q&A App

This project implements a **Retrieval-Augmented Generation (RAG)** system with a Streamlit frontend and modular backend, enabling users to upload a variety of document types (PDF, TXT, DOCX, PPTX, Excel, CSV) and ask questions over their content.
It uses LangChain + ChromaDB + OpenAI for embeddings & LLM, and integrates LangSmith for tracing and observability of your pipeline.

---

## ğŸ“‚ File Structure

```
rag-streamlit/
â”œâ”€ .env.example
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ .streamlit/
â”‚  â””â”€ config.toml
â”œâ”€ chroma_db/                # Persistent vector store (Chroma)
â”œâ”€ app.py                    # Streamlit UI entrypoint
â”œâ”€ src/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ config.py              # env + settings handling (including LangSmith)
â”‚  â”œâ”€ ingest.py              # ingestion & loaders (PDF, TXT, DOCX, Excel, CSV)
â”‚  â”œâ”€ vectorstore.py         # ChromaDB wrapper logic
â”‚  â”œâ”€ chains.py              # retrieval + QA chain logic
â”‚  â””â”€ utils.py               # helpers (file handling, text splitting, etc.)
â””â”€ notebooks/
   â””â”€ quick_test.ipynb       # optional exploratory notebook
```

---



## ğŸ” System Architecture & Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   .env file   â”‚ â†’ defines OPENAI, LANGSMITH and other settings
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Settings (config)  â”‚ â†’ loads envs, sets defaults for models, tracing
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit UI (app.py)       â”‚ â†’ upload docs, ingestion trigger, ask queries
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ uploads docs
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ingestion (ingest.py)             â”‚ â†’ loads PDF/TXT/DOCX/Excel/CSV, splits text
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ docsâ†’chunks
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vector Store (vectorstore.py)        â”‚ â†’ embed chunks, store/retrieve via ChromaDB
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ retrieval
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   QA Chain (chains.py)                â”‚ â†’ uses retriever + OpenAI LLM to answer
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ answers
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit Output (app.py)            â”‚ â†’ show answer + source snippets
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Features

* âœ… Multi-file ingestion: PDF, TXT, MD, DOCX, PPTX, Excel (.xlsx/.xls) and CSV.
* âœ… Persistent vector store via ChromaDB (`chroma_db/`) for reuse of embeddings across sessions.
* âœ… Natural language Q&A powered by OpenAI LLMs.
* âœ… Full observability: integrated with LangSmith to trace ingestion, embedding, retrieval & LLM steps.
* âœ… Modular architecture: separated config, ingestion, vectorstore, chain, UI for maintainability & extensibility.
* âœ… Adjustable parameters: chunk size, overlap, model names, and tracing toggles via environment.

---

## LangSmith Observability

* You can enable tracing by setting `LANGSMITH_TRACING=true` and `LANGSMITH_API_KEY` environment variables. ([docs.smith.langchain.com][1])
* Use the `@traceable` decorator or context manager from the LangSmith SDK to trace functions, chains or entire pipelines. ([docs.smith.langchain.com][2])
* In the LangSmith UI youâ€™ll see spans for each step: ingestion, embedding, retrieval, LLM call â€” grouped under a trace representing a user query. ([docs.smith.langchain.com][3])

---

## ğŸ§© Module Overview

| Module           | Description                                                                  |
| ---------------- | ---------------------------------------------------------------------------- |
| `config.py`      | Loads `.env`, sets up models, vector store path, LangSmith tracing config.   |
| `ingest.py`      | Document loaders + chunking logic (including Excel/CSV support).             |
| `vectorstore.py` | Handles embedding generation + persistent storage & similarity search.       |
| `chains.py`      | Builds the retrieval + LLM QA chain pipeline.                                |
| `utils.py`       | Helper utilities (file handling, chunking splitter, format detection).       |
| `app.py`         | Streamlit UI: file upload, ingestion, query input, answer & sources display. |
